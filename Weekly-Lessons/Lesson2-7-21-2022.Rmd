---
title: "Lesson2-7-21-2022"
author: "Aaron Mamula"
date: "7/20/2022"
output: html_document
---

# Outline

1. Recap from Lesson 1: reading external data and writing a first script
2. RStudio Projects
3. Libraries and Packages
4. Data Types & Data Structures

# RStudio Projects

[Here is a youtube video](https://youtu.be/bmUwgpqLVSw) that I made for another class on how to create a new project in RStudio. The audio is poor and it's longer than it needs to be but if you have 8 minutes to spare, bump the playback speed up to 1.5 (because I'm a slow talker) and it will show you how to create a new project.

There are many good reasons to organize your work in projects. You can read a more thorough discussion the advantages of organizing work as project [in this Software Carpentry Module](https://swcarpentry.github.io/r-novice-gapminder/02-project-intro/index.html). I'm not qualified to talk about all the magnificent ways that RStudio Projects will change your lives, I'm only really qualified to discuss why I find them advantageous:

1. Organizing work in projects makes it self-contained which makes it easily reproducible and shareable.


Let's try an example here:

* navigate to [the Github Repository for this HPP R Club](https://github.com/aaronmams/HPP-R-Group-2022)
* click the green button that says "Code"
* click "download Zip"
* Create an RStudio Project
* use your OS file browser to locate the downloaded zip file, move it to some location on your computer, and unzip the file
* move the two unzipped resources "Weekly-Lessons" and "data" into your RStudio project

## Bad Workflow Example

Here is an example of the kinds of workflow problems that projects, and more specifically for us RStudio Projects, are meant to avoid:

* Imagine that we have our Shasta Lake Level data in an Excel Workbook
* we want to do some analysis of seasonal flows over time so we create a new sheet and make some tables of summary statistics.
* Next we decide that daily data are too noisy and what we really want is just some reasonably smooth data value that illustrates trend. We decide to use the weekly maximum value. To do this we create a new column in the sheet called "week_max" and decide to define a week as Sunday - Saturday so every row corresponding to a Sunday has an observation and every other cell is blank.
* Next we get some data on temperature of the Sacramento River. We want to match this temperature data with Shasta Lake Level data to determine how releases from Shasta Dam affect the temperature of the river.
* We create a new sheet and put our temperature data there
* The temperature data is collected every hour so there are 24 observations for every day. Our lake level data is daily so we need to create a temperature value that is daily. We decide to use the daily max value so we create a new column for this called "daily_max." 
* But to summarize how temperature is related to lake level we need the change in daily lake level. That is, the amount of water released from the dam should equal the lake level yesterday - lake level today. So we make another column.
* Now we make another sheet for our table of summary statistics. 


I would like to highlight two very common problems that arise with this type of workflow:

1. When the number of data manipulations is small, the researcher can probably keep track of each one if they are meticulous about documenting each step. When the number of data manipulations grows so does the probability that one of these operations won't be properly documented and the researcher will end up with a set of tables with numbers in them, the origin of which is totally unknown.

As a corollary, my observations have lead me to conclude that virtually everyone (myself included) overestimates how meticulous they really are. If you believe that you will painstakingly record and document every data manipulation and nothing will slip through the cracks no matter how much the project scope grows, I have 15 years of experience working with people that love Excel that tell me you're wrong. 

2. If there is only 1 person working on this project maybe it's possible to document every single data manipulation properly. But what if two people are working on the project. Imagine that one of those people makes a table that summarizes the weekly maximum lake level value over time. Then the other person thinks it's strange that the lake level values are in feet instead of meters and decides to convert them. This changes all the values in the tables that are linked to the data. And what if one of those tables has already been copied over to a Microsoft Word document for a report? Now we have a table in the Excel file that should match the table that copied into the report but does not.


# Libraries and Packages

## How to Install Packages

From a programming and software development standpoint I think it is frowned upon to introduce this topic this early in the tutorial. From a scientific inquiry and data analysis standpoint I think this topic is critical and needs to be addressed sooner rather than later.

The process for installing packages is pretty straightforward. Normally one can just type:

```{r, eval=F}
install.packages("packagename")

```

If you copy that line into your R console it will probably tell you there is no package called "package name" but you may be able to execute this line:

```{r, eval=F}
install.packages("dplyr")

```

Once a package is installed, it still must be loaded into the active workspace. This can be done as follows:

```{r}
require(dplyr)

```

Sometimes, weird things happen and packages don't install. Sometimes it can be beneficial to try the point-and-click way:

* Packages --> install packages if you are using the R Gui
* Tools --> install packages in RStudio


Sometimes, installing and using packages gets funky if you don't understand exactly where R is installing and looking for external packages. In such cases, a good first step is to look at where R is looking for packages:

```{r}
.libPaths()

```

This will give you the file locations where R is looking to find packages when you do a ```require(packagename)```. 

In cases where the place that R is installing packages when you do ```install.packages(packagename)``` is not the same place that R is looking for packages when you do ```require(packagename)```, the solution to overcome this will be a little difficult. 

[I suggest starting here](https://www.accelebrate.com/library/how-to-articles/r-rstudio-library). I am also available to help troubleshoot these types of issues.


## Why Do I Need to Install Packages

I encourage you to do some of your own reading and form your own opinions about how much you want to rely on external packages versus base code. The R approach of having a lot functionality "baked in" to base code then allowing lots of other functionalities to be supported by other open source libraries has the following flexibility:

it makes R a very flexible language while still being a relatively easy to use language. In the world of computers and code there will always be a tradeoff between scope (the universe of things you can do) and approachability (how easy it is to do things).

C is a language with a lot of flexibility. You can do anything from complex biostatistics to building an iPhone app in C. However, doing anything C requires substantial expertise in both the subject matter and the programming language. You *can* take the mean of a group of numbers in C but you can't tell C something like:

```{r}
mean(2,3,4)

```

Instead you would do something like,

```{r, eval=F}
int i, j, k; 
double meanval;

i=10;
j=20;
k=30;

meanval = (i+j+k)/3

```

On the other hand, Excel makes it very easy to take the mean value of some numbers. You just point-and-click the *mean* function and highlight the cells you want to use.

To recap:

* C has a lot of flexibility but also a lot of complexity. In C you are not limited only to the things C says you can do, you are only limited by the things you are able to do.
* Excel has no flexibility but also very little complexity. In Excel you are completely limited only to operations that Excel approves of - you can take the mean of some number, you can make a bar chart, but you will never be able to program an iPhone app.

R's approach to navigate this tradeoff between flexibility and complexity is to offer some things you can do with base code - make graphs, add numbers, store data - and leave other functionalities to be developed by the users. The result is a very powerful piece of software that can do highly complex analysis across a wide range of disciplines (finance, political science, biology, hydrology, oceanography, physics, psychology, etc.) but is still relatively easy to use. 


Some External Resources:

* [Here is a list of popular packages hosted by RStudio](https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages)
* [An entire book on R Packages by Hadley Wickham and Jenny Bryan](https://r-pkgs.org/)



# Data Types and Data Structures

This is a weird transition, I know. Given the mix of scholars in the HPP Program this year I think it's good to mix procedural content with some technical content. The previous sections were kind procedural, focused on the basic question of "how do we do *things* in R." This exercise is a little more technical, focused more on the question of, "how do we analyze data in R." 

I have a [full tutorial on this topic available here](https://drive.google.com/file/d/1ecS0OVft4oZRfmC2-cFO8GUPQnqBaNLs/view?usp=sharing). It's kind of long but it has lots of code that you can use to familiarize yourself with data structures in R.



